'''
https://contest.yandex.ru/contest/24414/run-report/116263647/


Задача:
1) построить поисковый индекс по документам;
2) осуществить поиск и вывод пяти наиболее релевантных документов для заданных запросов.

-- ПРИНЦИП РАБОТЫ –
Для начала получим входные данные
- документы просто складываются в список документов,
- а каждое слово в запросе добавляется в список, только если оно не было добавлено ранее,
чтобы избежать дубликатов; для этого используется множество.

Согласно условию, при решении задачи можно использовать хеш-таблицы из стандартных библиотек.

Для реализации поискового индекса воспользуемся defaultdict
из стандартной библиотеки collections, который отлично подходит
для задач связанных с подсчетом частот.

defaultdict автоматически создает значения для новых ключей,
если ключ отсутствует в словаре, и присваивает ему значение по умолчанию.
Это делает код читабельнее и лаконичнее. Впрочем, следует помнить, что,
если попытаться получить доступ к несуществующему ключу,
defaultdict создаст его со значением по умолчанию, а это может привести
к созданию ненужных ключей и, как следствие, нерациональному расходованию памяти.

Для решения нашей задачи создадим defaultdict(lambda: defaultdict(int)):
- внешний словарь будет хранить слова как ключи,
- внутренний – идентификаторы документов (doc_id) как ключи
  и количество вхождений слова в этот документ как значения.
Для генерации doc_id воспользуемся функцией enumerate(docs),
которая позволяет одновременно итерироваться по элементам списка docs и получать для них индекс.
Для каждого документа doc_id разобьем документ на слова
и для каждого слова в этом документе будем увеличивать счетчик в словаре.

Для поиска наиболее релевантных документов, для каждого запроса из массива searching_words,
будем создавать временный словарь для хранения релевантных документов
Для каждого слова в запросе проверяем его наличие в хранилище,
если слово найдено, увеличиваем счетчик релевантности соответствующего документа.

Для сортировки релевантных документов используем функцию nlargest модуля heapq
из стандартной библиотеки, которая возвращает заданное число наибольших элементов
из итерируемого объекта, сортируя их сначала по значению в порядке убывания,
а затем по ключу в порядке возрастания для одинаковых значений.

-- ДОКАЗАТЕЛЬСТВО КОРРЕКТНОСТИ --
Корректность работы алгоритма обеспечивается подсчетом числа вхождений
для каждого уникального слова запроса в каждый документ.
Документы сортируются по убыванию релевантности,
а в случае их совпадения, по возрастанию идентификаторов.

-- ВРЕМЕННАЯ СЛОЖНОСТЬ --
Временная сложность построения поискового индекса включает:
- построение поискового индекса по документам, которая осуществляется за O(n*m),
  где n — количество документов, m — среднее количество слов в документе.

  Поисковый индекс строится один раз и не зависит от числа запросов.

- поиск наиболее релевантных документов по запросу и их сортировка,
  которые осуществляются за O(r * w),
  где r — количество запросов, w — среднее количество слов в запросе;
  скорость поиска элемента по ключу в словаре в среднем осуществляется за О(1);
  и сортировки, которая осуществляется за O(n log k),
  где n - общее количество объектов, k - количество элементов, которые нужно найти.


Рассмотрим крайние случаи, когда:
· запросы состоят из слов, встречающихся в малом количестве документов
· и когда одно слово много раз встречается в одном документе.

Если отбросить время построения хеш-таблицы, то:
в первом случае время работы будет складываться из:
· O(Q), где Q — суммарная длина всех слов в запросе
(для каждого слова поиск в словаре выполняется за O(1)
· и O(D), где D — число документов, содержащих слова из запроса;
если слова встречаются в малом количестве документов,
значит D будет маленьким и цикл по doc_id будет коротким;
· сортировку релевантных документов будем считать как O(1),
так как число документов для сортировки мало.
Следовательно, временная сложность случая, когда запросы состоят из редких слов, составит O(Q)+O(D).

во втором случае, когда запрос состоит из слов, которые
часто встречаются в документах много раз, время работы будет складываться из:
· O(Q), где Q — суммарная длина всех слов в запросе
(для каждого слова поиск в словаре выполняется за O(1), как и в первом случае
· и O(D), где D — число документов, содержащих слова из запроса;
если одно слово много раз встречается в одном документе,
все равно количество документов, в которых слово встречается, остается прежним;
· Время сортировки числа релевантных документов будем считать за O(Dr).
Следовательно, можем считать, что временная сложность случая,
когда запросы состоят из слов, которые много раз встречается в одном документе составляет
O(Q)+O(D)+O(Dr), где
Q — суммарная длина всех слов в запросе (для каждого слова поиск в словаре
выполняется за O(1) как и в первом случае
D — число документов, содержащих слова из запроса
Dr — число релевантных документов.

В обоих случаях Q — суммарная длина всех слов в запросе, D — число документов,
содержащих слова из запроса являются определяющими.

Отдельно, следует рассмотреть вариант, когда в запросах встречаются “стоп-слова”
(предлоги, артикли, местоимения). В этом случае рекомендуется добавить
в решение фильтрацию “стоп-слов”, чтобы слишком часто употребимые слова
не замедляли процесс и не увеличивали размер поискового индекса.

-- ПРОСТРАНСТВЕННАЯ СЛОЖНОСТЬ --
Пространственная сложность составляет O(n * m),
где n — количество документов, m — среднее количество слов в документе для хранения словаря storage.
Дополнительно используется память для хранения временных словарей
и списка релевантных документов и зависит от количества уникальных слов в запросе.
'''


from collections import defaultdict
import heapq


def docs_to_storage(docs):
    storage = defaultdict(lambda: defaultdict(int))

    for doc_id, line in enumerate(docs, start=1):
        words = line.split()
        for word in words:
            storage[word][doc_id] += 1

    return storage


def get_most_relevant_docs(storage, searching_words):
    relevant_docs = []

    for query in searching_words:
        temp_result = defaultdict(int)
        for word in query:
            if word in storage:
                for doc_id, count in storage[word].items():
                    temp_result[doc_id] += count

        top_docs = heapq.nlargest(5, temp_result.items(), key=lambda x: (x[1], -x[0]))
        relevant_docs.append([doc_id for doc_id, _ in top_docs])
    return relevant_docs


def read_input():
    n = int(input().strip())
    docs = [input().strip() for _ in range(n)]

    m = int(input().strip())
    searching_words = []
    for _ in range(m):
        temp = input().strip().split()
        temp_line = list(set(temp))
        searching_words.append(temp_line)

    return docs, searching_words


if __name__ == '__main__':
    docs, searching_words = read_input()
    storage = docs_to_storage(docs)
    relevant_docs = get_most_relevant_docs(storage, searching_words)

    for result in relevant_docs:
        print(*result)
